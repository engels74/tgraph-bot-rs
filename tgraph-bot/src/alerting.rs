//! Alerting system for monitoring task failures and system health
//!
//! This module provides configurable alerting for repeated task failures,
//! performance degradation, and system health issues.

use anyhow::{Context, Result};
use chrono::{DateTime, Duration as ChronoDuration, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::metrics::{AggregatedMetrics, MetricsCollector, TaskExecutionMetric};

/// Unique identifier for an alert rule
pub type AlertRuleId = Uuid;

/// Unique identifier for an alert instance
pub type AlertId = Uuid;

/// Severity levels for alerts
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum AlertSeverity {
    /// Low priority alerts for minor issues
    Low,
    /// Medium priority alerts for moderate issues
    Medium,
    /// High priority alerts for serious issues
    High,
    /// Critical alerts for system-threatening issues
    Critical,
}

impl Default for AlertSeverity {
    fn default() -> Self {
        AlertSeverity::Medium
    }
}

/// Types of alert conditions
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum AlertCondition {
    /// Alert when failure rate exceeds threshold
    FailureRate {
        /// Threshold percentage (0.0 to 100.0)
        threshold_percent: f64,
        /// Time window to evaluate
        window_minutes: u32,
        /// Minimum number of executions required
        min_executions: u32,
    },
    /// Alert when consecutive failures exceed threshold
    ConsecutiveFailures {
        /// Number of consecutive failures to trigger alert
        threshold: u32,
    },
    /// Alert when execution duration exceeds threshold
    ExecutionDuration {
        /// Maximum allowed duration in seconds
        threshold_seconds: u64,
        /// Number of violations in window to trigger
        violations_in_window: u32,
        /// Time window in minutes
        window_minutes: u32,
    },
    /// Alert when no executions occur within expected timeframe
    MissingExecution {
        /// Expected execution interval in minutes
        expected_interval_minutes: u32,
        /// Grace period before alerting in minutes
        grace_period_minutes: u32,
    },
    /// Alert when retry rate is too high
    HighRetryRate {
        /// Threshold for retries per execution
        threshold_ratio: f64,
        /// Time window to evaluate
        window_minutes: u32,
        /// Minimum number of executions required
        min_executions: u32,
    },
}

/// Configuration for an alert rule
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertRule {
    /// Unique identifier for this rule
    pub id: AlertRuleId,
    /// Human-readable name for the rule
    pub name: String,
    /// Optional description of what this rule monitors
    pub description: Option<String>,
    /// Task type to monitor (empty string for all tasks)
    pub task_type_filter: String,
    /// Specific task name to monitor (empty string for all tasks)
    pub task_name_filter: String,
    /// Condition that triggers this alert
    pub condition: AlertCondition,
    /// Severity level of alerts generated by this rule
    pub severity: AlertSeverity,
    /// Whether this rule is currently enabled
    pub enabled: bool,
    /// Cooldown period between alerts in minutes
    pub cooldown_minutes: u32,
    /// Notification channels for this alert
    pub notification_channels: Vec<String>,
    /// When this rule was created
    pub created_at: DateTime<Utc>,
    /// When this rule was last modified
    pub updated_at: DateTime<Utc>,
}

impl AlertRule {
    /// Create a new alert rule
    pub fn new(
        name: String,
        task_type_filter: String,
        task_name_filter: String,
        condition: AlertCondition,
        severity: AlertSeverity,
        notification_channels: Vec<String>,
    ) -> Self {
        let now = Utc::now();
        Self {
            id: Uuid::new_v4(),
            name,
            description: None,
            task_type_filter,
            task_name_filter,
            condition,
            severity,
            enabled: true,
            cooldown_minutes: 15, // Default 15 minute cooldown
            notification_channels,
            created_at: now,
            updated_at: now,
        }
    }

    /// Check if this rule applies to a given task
    pub fn matches_task(&self, task_type: &str, task_name: &str) -> bool {
        let type_matches = self.task_type_filter.is_empty() || self.task_type_filter == task_type;
        let name_matches = self.task_name_filter.is_empty() || self.task_name_filter == task_name;
        type_matches && name_matches
    }
}

/// An active alert instance
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Alert {
    /// Unique identifier for this alert
    pub id: AlertId,
    /// ID of the rule that generated this alert
    pub rule_id: AlertRuleId,
    /// Name of the rule that generated this alert
    pub rule_name: String,
    /// Task type that triggered the alert
    pub task_type: String,
    /// Task name that triggered the alert (if specific)
    pub task_name: Option<String>,
    /// Severity of this alert
    pub severity: AlertSeverity,
    /// Human-readable message describing the alert
    pub message: String,
    /// Additional context data
    pub context: HashMap<String, serde_json::Value>,
    /// When this alert was first triggered
    pub triggered_at: DateTime<Utc>,
    /// When this alert was last updated
    pub updated_at: DateTime<Utc>,
    /// Whether this alert has been acknowledged
    pub acknowledged: bool,
    /// When this alert was acknowledged (if applicable)
    pub acknowledged_at: Option<DateTime<Utc>>,
    /// Who acknowledged this alert
    pub acknowledged_by: Option<String>,
    /// Whether this alert has been resolved
    pub resolved: bool,
    /// When this alert was resolved (if applicable)
    pub resolved_at: Option<DateTime<Utc>>,
    /// How this alert was resolved
    pub resolution_reason: Option<String>,
}

impl Alert {
    /// Create a new alert from a rule and context
    pub fn new(
        rule: &AlertRule,
        task_type: String,
        task_name: Option<String>,
        message: String,
        context: HashMap<String, serde_json::Value>,
    ) -> Self {
        let now = Utc::now();
        Self {
            id: Uuid::new_v4(),
            rule_id: rule.id,
            rule_name: rule.name.clone(),
            task_type,
            task_name,
            severity: rule.severity,
            message,
            context,
            triggered_at: now,
            updated_at: now,
            acknowledged: false,
            acknowledged_at: None,
            acknowledged_by: None,
            resolved: false,
            resolved_at: None,
            resolution_reason: None,
        }
    }

    /// Acknowledge this alert
    pub fn acknowledge(&mut self, acknowledged_by: String) {
        self.acknowledged = true;
        self.acknowledged_at = Some(Utc::now());
        self.acknowledged_by = Some(acknowledged_by);
        self.updated_at = Utc::now();
    }

    /// Resolve this alert
    pub fn resolve(&mut self, reason: String) {
        self.resolved = true;
        self.resolved_at = Some(Utc::now());
        self.resolution_reason = Some(reason);
        self.updated_at = Utc::now();
    }

    /// Check if this alert is active (not acknowledged and not resolved)
    pub fn is_active(&self) -> bool {
        !self.acknowledged && !self.resolved
    }
}

/// Notification channel configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NotificationChannel {
    /// Unique name for this channel
    pub name: String,
    /// Type of notification channel
    pub channel_type: NotificationChannelType,
    /// Configuration specific to the channel type
    pub config: HashMap<String, String>,
    /// Whether this channel is enabled
    pub enabled: bool,
}

/// Types of notification channels
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum NotificationChannelType {
    /// Log-based notifications
    Log,
    /// Email notifications
    Email {
        /// SMTP server configuration
        smtp_server: String,
        /// SMTP port
        smtp_port: u16,
        /// Username for SMTP authentication
        username: String,
        /// Recipients for alerts
        recipients: Vec<String>,
    },
    /// Webhook notifications
    Webhook {
        /// URL to send webhook to
        url: String,
        /// HTTP method to use
        method: String,
        /// Additional headers
        headers: HashMap<String, String>,
    },
    /// Discord webhook notifications
    Discord {
        /// Discord webhook URL
        webhook_url: String,
    },
    /// Slack webhook notifications
    Slack {
        /// Slack webhook URL
        webhook_url: String,
        /// Channel to post to
        channel: String,
    },
}

/// Alert manager that evaluates rules and manages alerts
pub struct AlertManager {
    /// Alert rules indexed by ID
    rules: Arc<RwLock<HashMap<AlertRuleId, AlertRule>>>,
    /// Active alerts indexed by ID
    alerts: Arc<RwLock<HashMap<AlertId, Alert>>>,
    /// Notification channels indexed by name
    channels: Arc<RwLock<HashMap<String, NotificationChannel>>>,
    /// Metrics collector for evaluating conditions
    metrics_collector: Arc<MetricsCollector>,
    /// Cooldown tracking for rules
    rule_cooldowns: Arc<RwLock<HashMap<AlertRuleId, DateTime<Utc>>>>,
}

impl AlertManager {
    /// Create a new alert manager
    pub fn new(metrics_collector: Arc<MetricsCollector>) -> Self {
        info!("Creating new alert manager");
        
        Self {
            rules: Arc::new(RwLock::new(HashMap::new())),
            alerts: Arc::new(RwLock::new(HashMap::new())),
            channels: Arc::new(RwLock::new(HashMap::new())),
            metrics_collector,
            rule_cooldowns: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    /// Add a new alert rule
    pub async fn add_rule(&self, mut rule: AlertRule) -> Result<AlertRuleId> {
        rule.updated_at = Utc::now();
        let rule_id = rule.id;
        
        let mut rules = self.rules.write().await;
        rules.insert(rule_id, rule.clone());
        
        info!("Added alert rule: {} ({})", rule.name, rule_id);
        Ok(rule_id)
    }

    /// Remove an alert rule
    pub async fn remove_rule(&self, rule_id: AlertRuleId) -> Result<()> {
        let mut rules = self.rules.write().await;
        if rules.remove(&rule_id).is_some() {
            info!("Removed alert rule: {}", rule_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Alert rule not found: {}", rule_id))
        }
    }

    /// Get all alert rules
    pub async fn get_rules(&self) -> Vec<AlertRule> {
        let rules = self.rules.read().await;
        rules.values().cloned().collect()
    }

    /// Get a specific alert rule
    pub async fn get_rule(&self, rule_id: AlertRuleId) -> Option<AlertRule> {
        let rules = self.rules.read().await;
        rules.get(&rule_id).cloned()
    }

    /// Add a notification channel
    pub async fn add_channel(&self, channel: NotificationChannel) -> Result<()> {
        let mut channels = self.channels.write().await;
        channels.insert(channel.name.clone(), channel.clone());
        info!("Added notification channel: {}", channel.name);
        Ok(())
    }

    /// Get all notification channels
    pub async fn get_channels(&self) -> Vec<NotificationChannel> {
        let channels = self.channels.read().await;
        channels.values().cloned().collect()
    }

    /// Evaluate all alert rules against current metrics
    pub async fn evaluate_rules(&self) -> Result<Vec<Alert>> {
        let rules = self.rules.read().await;
        let mut new_alerts = Vec::new();

        for rule in rules.values() {
            if !rule.enabled {
                continue;
            }

            // Check cooldown
            if self.is_rule_in_cooldown(rule.id).await {
                continue;
            }

            // Evaluate the rule condition
            if let Some(alert) = self.evaluate_rule_condition(rule).await? {
                new_alerts.push(alert);

                // Set cooldown for this rule
                let mut cooldowns = self.rule_cooldowns.write().await;
                cooldowns.insert(rule.id, Utc::now());
            }
        }

        // Store new alerts
        if !new_alerts.is_empty() {
            let mut alerts = self.alerts.write().await;
            for alert in &new_alerts {
                alerts.insert(alert.id, alert.clone());
            }
            info!("Generated {} new alerts", new_alerts.len());
        }

        Ok(new_alerts)
    }

    /// Check if a rule is currently in cooldown
    async fn is_rule_in_cooldown(&self, rule_id: AlertRuleId) -> bool {
        let cooldowns = self.rule_cooldowns.read().await;
        if let Some(last_alert_time) = cooldowns.get(&rule_id) {
            let rules = self.rules.read().await;
            if let Some(rule) = rules.get(&rule_id) {
                let cooldown_duration = ChronoDuration::minutes(rule.cooldown_minutes as i64);
                return Utc::now() - *last_alert_time < cooldown_duration;
            }
        }
        false
    }

    /// Evaluate a specific rule condition
    async fn evaluate_rule_condition(&self, rule: &AlertRule) -> Result<Option<Alert>> {
        match &rule.condition {
            AlertCondition::FailureRate { threshold_percent, window_minutes, min_executions } => {
                self.evaluate_failure_rate_condition(rule, *threshold_percent, *window_minutes, *min_executions).await
            }
            AlertCondition::ConsecutiveFailures { threshold } => {
                self.evaluate_consecutive_failures_condition(rule, *threshold).await
            }
            AlertCondition::ExecutionDuration { threshold_seconds, violations_in_window, window_minutes } => {
                self.evaluate_execution_duration_condition(rule, *threshold_seconds, *violations_in_window, *window_minutes).await
            }
            AlertCondition::MissingExecution { expected_interval_minutes, grace_period_minutes } => {
                self.evaluate_missing_execution_condition(rule, *expected_interval_minutes, *grace_period_minutes).await
            }
            AlertCondition::HighRetryRate { threshold_ratio, window_minutes, min_executions } => {
                self.evaluate_high_retry_rate_condition(rule, *threshold_ratio, *window_minutes, *min_executions).await
            }
        }
    }

    /// Evaluate failure rate condition
    async fn evaluate_failure_rate_condition(
        &self,
        rule: &AlertRule,
        threshold_percent: f64,
        window_minutes: u32,
        min_executions: u32,
    ) -> Result<Option<Alert>> {
        let end_time = Utc::now();
        let start_time = end_time - ChronoDuration::minutes(window_minutes as i64);

        // Get metrics for the time window
        let task_types = if rule.task_type_filter.is_empty() {
            // Get all unique task types
            let recent_metrics = self.metrics_collector.get_recent_metrics(1000).await;
            let mut types: std::collections::HashSet<String> = recent_metrics.iter().map(|m| m.task_type.clone()).collect();
            types.into_iter().collect()
        } else {
            vec![rule.task_type_filter.clone()]
        };

        for task_type in task_types {
            let aggregated = self.metrics_collector
                .calculate_aggregated_metrics(&task_type, start_time, end_time)
                .await;

            if aggregated.total_executions >= min_executions as u64 &&
               aggregated.success_rate < (100.0 - threshold_percent) {

                let mut context = HashMap::new();
                context.insert("failure_rate".to_string(), serde_json::Value::Number(
                    serde_json::Number::from_f64(100.0 - aggregated.success_rate).unwrap()
                ));
                context.insert("threshold".to_string(), serde_json::Value::Number(
                    serde_json::Number::from_f64(threshold_percent).unwrap()
                ));
                context.insert("total_executions".to_string(), serde_json::Value::Number(
                    serde_json::Number::from(aggregated.total_executions)
                ));
                context.insert("failed_executions".to_string(), serde_json::Value::Number(
                    serde_json::Number::from(aggregated.failed_executions)
                ));

                let message = format!(
                    "High failure rate detected for task type '{}': {:.1}% failure rate ({} failures out of {} executions) exceeds threshold of {:.1}%",
                    task_type,
                    100.0 - aggregated.success_rate,
                    aggregated.failed_executions,
                    aggregated.total_executions,
                    threshold_percent
                );

                return Ok(Some(Alert::new(
                    rule,
                    task_type,
                    None,
                    message,
                    context,
                )));
            }
        }

        Ok(None)
    }

    /// Evaluate consecutive failures condition
    async fn evaluate_consecutive_failures_condition(
        &self,
        rule: &AlertRule,
        threshold: u32,
    ) -> Result<Option<Alert>> {
        let recent_metrics = self.metrics_collector.get_recent_metrics(threshold as usize * 2).await;

        // Filter metrics based on rule filters
        let filtered_metrics: Vec<_> = recent_metrics.iter()
            .filter(|m| rule.matches_task(&m.task_type, &m.task_name))
            .filter(|m| m.finished_at.is_some())
            .collect();

        if filtered_metrics.len() < threshold as usize {
            return Ok(None);
        }

        // Sort by start time (most recent first)
        let mut sorted_metrics = filtered_metrics;
        sorted_metrics.sort_by(|a, b| b.started_at.cmp(&a.started_at));

        // Check for consecutive failures
        let consecutive_failures = sorted_metrics.iter()
            .take(threshold as usize)
            .all(|m| m.success == Some(false));

        if consecutive_failures {
            let task_type = sorted_metrics[0].task_type.clone();
            let task_name = sorted_metrics[0].task_name.clone();

            let mut context = HashMap::new();
            context.insert("consecutive_failures".to_string(), serde_json::Value::Number(
                serde_json::Number::from(threshold)
            ));
            context.insert("threshold".to_string(), serde_json::Value::Number(
                serde_json::Number::from(threshold)
            ));

            let message = format!(
                "Consecutive failures detected for task '{}' (type: '{}'): {} consecutive failures exceeds threshold of {}",
                task_name,
                task_type,
                threshold,
                threshold
            );

            return Ok(Some(Alert::new(
                rule,
                task_type,
                Some(task_name),
                message,
                context,
            )));
        }

        Ok(None)
    }

    /// Evaluate execution duration condition
    async fn evaluate_execution_duration_condition(
        &self,
        rule: &AlertRule,
        threshold_seconds: u64,
        violations_in_window: u32,
        window_minutes: u32,
    ) -> Result<Option<Alert>> {
        let end_time = Utc::now();
        let start_time = end_time - ChronoDuration::minutes(window_minutes as i64);

        let recent_metrics = self.metrics_collector.get_recent_metrics(1000).await;

        // Filter metrics based on rule filters and time window
        let filtered_metrics: Vec<_> = recent_metrics.iter()
            .filter(|m| rule.matches_task(&m.task_type, &m.task_name))
            .filter(|m| m.started_at >= start_time && m.started_at <= end_time)
            .filter(|m| m.duration_ms.is_some())
            .collect();

        // Count violations (executions exceeding threshold)
        let violations = filtered_metrics.iter()
            .filter(|m| m.duration_ms.unwrap() > threshold_seconds * 1000)
            .count();

        if violations >= violations_in_window as usize {
            let task_type = if rule.task_type_filter.is_empty() {
                "all".to_string()
            } else {
                rule.task_type_filter.clone()
            };

            let mut context = HashMap::new();
            context.insert("violations".to_string(), serde_json::Value::Number(
                serde_json::Number::from(violations)
            ));
            context.insert("threshold_violations".to_string(), serde_json::Value::Number(
                serde_json::Number::from(violations_in_window)
            ));
            context.insert("duration_threshold_seconds".to_string(), serde_json::Value::Number(
                serde_json::Number::from(threshold_seconds)
            ));

            let message = format!(
                "Execution duration violations detected: {} executions exceeded {}s threshold (limit: {} violations in {}m window)",
                violations,
                threshold_seconds,
                violations_in_window,
                window_minutes
            );

            return Ok(Some(Alert::new(
                rule,
                task_type,
                None,
                message,
                context,
            )));
        }

        Ok(None)
    }

    /// Evaluate missing execution condition
    async fn evaluate_missing_execution_condition(
        &self,
        rule: &AlertRule,
        expected_interval_minutes: u32,
        grace_period_minutes: u32,
    ) -> Result<Option<Alert>> {
        let now = Utc::now();
        let expected_execution_time = now - ChronoDuration::minutes(expected_interval_minutes as i64);
        let grace_deadline = expected_execution_time - ChronoDuration::minutes(grace_period_minutes as i64);

        let recent_metrics = self.metrics_collector.get_recent_metrics(100).await;

        // Find the most recent execution matching the rule filters
        let last_execution = recent_metrics.iter()
            .filter(|m| rule.matches_task(&m.task_type, &m.task_name))
            .max_by_key(|m| m.started_at);

        let should_alert = match last_execution {
            Some(execution) => execution.started_at < grace_deadline,
            None => true, // No executions found at all
        };

        if should_alert {
            let task_type = if rule.task_type_filter.is_empty() {
                "all".to_string()
            } else {
                rule.task_type_filter.clone()
            };

            let mut context = HashMap::new();
            context.insert("expected_interval_minutes".to_string(), serde_json::Value::Number(
                serde_json::Number::from(expected_interval_minutes)
            ));
            context.insert("grace_period_minutes".to_string(), serde_json::Value::Number(
                serde_json::Number::from(grace_period_minutes)
            ));

            if let Some(execution) = last_execution {
                context.insert("last_execution".to_string(), serde_json::Value::String(
                    execution.started_at.to_rfc3339()
                ));
            }

            let message = if let Some(execution) = last_execution {
                format!(
                    "Missing expected execution: Last execution was at {}, expected every {}m (grace period: {}m)",
                    execution.started_at.format("%Y-%m-%d %H:%M:%S UTC"),
                    expected_interval_minutes,
                    grace_period_minutes
                )
            } else {
                format!(
                    "Missing expected execution: No executions found, expected every {}m (grace period: {}m)",
                    expected_interval_minutes,
                    grace_period_minutes
                )
            };

            return Ok(Some(Alert::new(
                rule,
                task_type,
                None,
                message,
                context,
            )));
        }

        Ok(None)
    }

    /// Evaluate high retry rate condition
    async fn evaluate_high_retry_rate_condition(
        &self,
        rule: &AlertRule,
        threshold_ratio: f64,
        window_minutes: u32,
        min_executions: u32,
    ) -> Result<Option<Alert>> {
        let end_time = Utc::now();
        let start_time = end_time - ChronoDuration::minutes(window_minutes as i64);

        let task_types = if rule.task_type_filter.is_empty() {
            let recent_metrics = self.metrics_collector.get_recent_metrics(1000).await;
            let mut types: std::collections::HashSet<String> = recent_metrics.iter().map(|m| m.task_type.clone()).collect();
            types.into_iter().collect()
        } else {
            vec![rule.task_type_filter.clone()]
        };

        for task_type in task_types {
            let aggregated = self.metrics_collector
                .calculate_aggregated_metrics(&task_type, start_time, end_time)
                .await;

            if aggregated.total_executions >= min_executions as u64 {
                let retry_ratio = aggregated.total_retries as f64 / aggregated.total_executions as f64;

                if retry_ratio > threshold_ratio {
                    let mut context = HashMap::new();
                    context.insert("retry_ratio".to_string(), serde_json::Value::Number(
                        serde_json::Number::from_f64(retry_ratio).unwrap()
                    ));
                    context.insert("threshold_ratio".to_string(), serde_json::Value::Number(
                        serde_json::Number::from_f64(threshold_ratio).unwrap()
                    ));
                    context.insert("total_retries".to_string(), serde_json::Value::Number(
                        serde_json::Number::from(aggregated.total_retries)
                    ));
                    context.insert("total_executions".to_string(), serde_json::Value::Number(
                        serde_json::Number::from(aggregated.total_executions)
                    ));

                    let message = format!(
                        "High retry rate detected for task type '{}': {:.2} retries per execution ({} retries, {} executions) exceeds threshold of {:.2}",
                        task_type,
                        retry_ratio,
                        aggregated.total_retries,
                        aggregated.total_executions,
                        threshold_ratio
                    );

                    return Ok(Some(Alert::new(
                        rule,
                        task_type,
                        None,
                        message,
                        context,
                    )));
                }
            }
        }

        Ok(None)
    }

    /// Get all active alerts
    pub async fn get_active_alerts(&self) -> Vec<Alert> {
        let alerts = self.alerts.read().await;
        alerts.values()
            .filter(|alert| alert.is_active())
            .cloned()
            .collect()
    }

    /// Get all alerts (active and resolved)
    pub async fn get_all_alerts(&self) -> Vec<Alert> {
        let alerts = self.alerts.read().await;
        alerts.values().cloned().collect()
    }

    /// Acknowledge an alert
    pub async fn acknowledge_alert(&self, alert_id: AlertId, acknowledged_by: String) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        if let Some(alert) = alerts.get_mut(&alert_id) {
            alert.acknowledge(acknowledged_by);
            info!("Acknowledged alert: {} ({})", alert.rule_name, alert_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Alert not found: {}", alert_id))
        }
    }

    /// Resolve an alert
    pub async fn resolve_alert(&self, alert_id: AlertId, reason: String) -> Result<()> {
        let mut alerts = self.alerts.write().await;
        if let Some(alert) = alerts.get_mut(&alert_id) {
            alert.resolve(reason);
            info!("Resolved alert: {} ({})", alert.rule_name, alert_id);
            Ok(())
        } else {
            Err(anyhow::anyhow!("Alert not found: {}", alert_id))
        }
    }
}
